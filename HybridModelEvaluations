import nltk
nltk.download('vader_lexicon')
import pandas as pd
from sklearn.model_selection import train_test_split
# Load your dataset
df = pd.read_csv('C:\\Users\\ethan\\Downloads\\Review, Sentiment - Sheet1
(2).csv')
# Assuming your dataset has the columns "text" for the review and
"sentiment" for the labels
X_train, X_test, y_train, y_test = train_test_split(df['review'],
df['sentiment'], test_size=0.3, random_state=42)
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline
model = make_pipeline(TfidfVectorizer(), MultinomialNB())
# Fit the model
model.fit(X_train, y_train)
# Predict with the model
ml_predictions = model.predict(X_test)
from nltk.sentiment import SentimentIntensityAnalyzer
sia = SentimentIntensityAnalyzer()
# Apply VADER to each review and decide on positive or negative based on
compound score
vader_predictions = [1 if sia.polarity_scores(text)['compound'] > 0 else 0
for text in X_test]



import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score,
f1_score
# Get consensus predictions
consensus_predictions = np.where(ml_predictions == vader_predictions,
ml_predictions, -1)
# Where consensus predictions are -1 (indicating disagreement), use ML
predictions
hybrid_predictions = np.where(consensus_predictions == -1, ml_predictions,
consensus_predictions)
# If there are no positive predictions, choose ML predictions for positive
reviews
if not np.any(hybrid_predictions == 1):
hybrid_predictions = ml_predictions
# Evaluate the hybrid model
print("Evaluation Metrics:")
print("Hybrid Model Accuracy:", accuracy_score(y_test,
hybrid_predictions))
print("Hybrid Model Precision:", precision_score(y_test,
hybrid_predictions))
print("Hybrid Model Recall:", recall_score(y_test, hybrid_predictions))
print("Hybrid Model F1 Score:", f1_score(y_test, hybrid_predictions,
average='binary'))
